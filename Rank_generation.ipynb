{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":13769,"status":"ok","timestamp":1759254743839,"user":{"displayName":"Cccttt C","userId":"09161705153548643246"},"user_tz":-540},"id":"T7DObxGPQC4A","outputId":"fee5e724-632b-41e6-9f29-77b1a6b0a83b"},"outputs":[{"output_type":"stream","name":"stdout","text":["2.8.0+cu126\n","0.23.0+cu126\n"]}],"source":["import matplotlib.pyplot as plt # for visualizing\n","import os\n","import torch\n","from torch import nn\n","import torchvision\n","from torchvision import datasets\n","from torchvision import transforms\n","from torchvision.transforms import ToTensor\n","import math\n","import torch.nn as nn\n","from collections import OrderedDict\n","\n","import torch.backends.cudnn as cudnn\n","import argparse\n","\n","\n","import numpy as np\n","\n","print(torch.__version__)\n","print(torchvision.__version__)"]},{"cell_type":"code","execution_count":2,"metadata":{"executionInfo":{"elapsed":506,"status":"ok","timestamp":1759254744409,"user":{"displayName":"Cccttt C","userId":"09161705153548643246"},"user_tz":-540},"id":"DXokBkAHbJ2x"},"outputs":[],"source":["device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"]},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":14443,"status":"ok","timestamp":1759254905960,"user":{"displayName":"Cccttt C","userId":"09161705153548643246"},"user_tz":-540},"id":"MvlFUwvLQuWP","outputId":"f2317a4e-efe2-405a-9f47-9daee3e8dd57"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":5,"metadata":{"executionInfo":{"elapsed":2,"status":"ok","timestamp":1759254905965,"user":{"displayName":"Cccttt C","userId":"09161705153548643246"},"user_tz":-540},"id":"EfUiJbcfQyLe"},"outputs":[],"source":["origin_dir = '/content/drive/MyDrive/Colab_Notebooks/URP/VGG16_CIFAR100_pruning'\n","data_dir = origin_dir + '/data'"]},{"cell_type":"code","execution_count":6,"metadata":{"executionInfo":{"elapsed":17466,"status":"ok","timestamp":1759254923433,"user":{"displayName":"Cccttt C","userId":"09161705153548643246"},"user_tz":-540},"id":"Gx9gyrXhQzVm"},"outputs":[],"source":["\n","transform_train = transforms.Compose([\n","    transforms.RandomCrop(32, padding=4),   # 데이터 증강: 랜덤 크롭\n","    transforms.RandomHorizontalFlip(),      # 좌우 반전\n","    transforms.AutoAugment(policy=transforms.AutoAugmentPolicy.CIFAR10),\n","    #데이터 증강 기법을 자동으로 탐색 후 적용\n","    transforms.ToTensor(),                  # Tensor 변환\n","    transforms.Normalize((0.5071, 0.4867, 0.4408),\n","                         (0.2675, 0.2565, 0.2761))\n","])\n","\n","transform_test = transforms.Compose([\n","    transforms.ToTensor(),\n","    transforms.Normalize((0.5071, 0.4867, 0.4408),\n","                         (0.2675, 0.2565, 0.2761)),\n","])\n","\n","trainset = torchvision.datasets.CIFAR100(\n","    root = data_dir, train=True, download=True, transform=transform_train\n",")\n","\n","testset = torchvision.datasets.CIFAR100(\n","    root= data_dir, train=False, download=True, transform=transform_test\n",")\n"]},{"cell_type":"code","execution_count":7,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":12,"status":"ok","timestamp":1759254923458,"user":{"displayName":"Cccttt C","userId":"09161705153548643246"},"user_tz":-540},"id":"FaunyGd9Q4c0","outputId":"1b1ae1c2-8a0c-4b50-bef6-95cccc3701f1"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["(<torch.utils.data.dataloader.DataLoader at 0x7f06b59efce0>,\n"," <torch.utils.data.dataloader.DataLoader at 0x7f06b5a1f980>)"]},"metadata":{},"execution_count":7}],"source":["trainloader = torch.utils.data.DataLoader(\n","    trainset, batch_size=128, shuffle=True, num_workers=2\n",")\n","\n","testloader = torch.utils.data.DataLoader(\n","    testset, batch_size=128, shuffle=False, num_workers=2\n",")\n","\n","trainloader,testloader"]},{"cell_type":"markdown","metadata":{"id":"BDE3lGbTScTT"},"source":["Model"]},{"cell_type":"code","execution_count":8,"metadata":{"executionInfo":{"elapsed":7,"status":"ok","timestamp":1759254923458,"user":{"displayName":"Cccttt C","userId":"09161705153548643246"},"user_tz":-540},"id":"_49UcuS2Sbsq"},"outputs":[],"source":["import math\n","import torch.nn as nn\n","from collections import OrderedDict\n","\n","norm_mean, norm_var = 0.0, 1.0\n","\n","defaultcfg = [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 'M', 512, 512, 512, 'M', 512, 512, 512, 512]\n","relucfg = [2, 6, 9, 13, 16, 19, 23, 26, 29, 33, 36, 39,42]\n","convcfg = [0, 3, 7, 10, 14, 17, 20, 24, 27, 30, 34, 37]\n","\n","\n","class VGG(nn.Module):\n","    def __init__(self, num_classes=100, init_weights=True, cfg=None):\n","        super(VGG, self).__init__()\n","        self.features = nn.Sequential()\n","\n","        if cfg is None:\n","            cfg = defaultcfg\n","\n","        self.relucfg = relucfg\n","        self.covcfg = convcfg\n","        self.features = self.make_layers(cfg[:-1], True)\n","        self.classifier = nn.Sequential(OrderedDict([\n","            ('linear1', nn.Linear(cfg[-2], cfg[-1])),\n","            ('norm1', nn.BatchNorm1d(cfg[-1])),\n","            ('relu1', nn.ReLU(inplace=True)),\n","            ('linear2', nn.Linear(cfg[-1], num_classes)),\n","        ]))\n","\n","        if init_weights:            #weight 초기화\n","            self._initialize_weights()\n","\n","    def make_layers(self, cfg, batch_norm=True):\n","        layers = nn.Sequential()\n","        in_channels = 3\n","        cnt = 0\n","        for i, v in enumerate(cfg):\n","            if v == 'M':\n","                layers.add_module('pool%d' % i, nn.MaxPool2d(kernel_size=2, stride=2))\n","            else:\n","                conv2d = nn.Conv2d(in_channels, v, kernel_size=3, padding=1)\n","\n","                layers.add_module('conv%d' % i, conv2d)\n","                layers.add_module('norm%d' % i, nn.BatchNorm2d(v))\n","                layers.add_module('relu%d' % i, nn.ReLU(inplace=True))\n","                in_channels = v\n","        return layers\n","\n","    def forward(self, x):\n","        x = self.features(x)\n","\n","        x = nn.AvgPool2d(2)(x)\n","        x = x.view(x.size(0), -1)\n","        x = self.classifier(x)\n","        return x\n","\n","    def _initialize_weights(self):\n","        for m in self.modules():\n","            if isinstance(m, nn.Conv2d):\n","                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n","                m.weight.data.normal_(0, math.sqrt(2. / n))\n","                if m.bias is not None:\n","                    m.bias.data.zero_()\n","            elif isinstance(m, nn.BatchNorm2d):\n","                m.weight.data.fill_(0.5)\n","                m.bias.data.zero_()\n","            elif isinstance(m, nn.Linear):\n","                m.weight.data.normal_(0, 0.01)\n","                m.bias.data.zero_()"]},{"cell_type":"code","execution_count":9,"metadata":{"executionInfo":{"elapsed":339,"status":"ok","timestamp":1759254923794,"user":{"displayName":"Cccttt C","userId":"09161705153548643246"},"user_tz":-540},"id":"IevOCNWESWQC"},"outputs":[],"source":["net = VGG()\n","net = net.to(device)"]},{"cell_type":"code","execution_count":10,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4168,"status":"ok","timestamp":1759254927972,"user":{"displayName":"Cccttt C","userId":"09161705153548643246"},"user_tz":-540},"id":"soYqMDbISCeX","outputId":"4ca5d8e8-4a86-4791-c21b-a81b81d2e8c0"},"outputs":[{"output_type":"stream","name":"stdout","text":["==> Building model..\n","==> Resuming from checkpoint..\n"]}],"source":["print('==> Building model..')\n","pre_train_model = origin_dir + '/original_model/model_epoch_120.pth'\n","\n","# Load checkpoint.\n","print('==> Resuming from checkpoint..')\n","checkpoint = torch.load(pre_train_model, map_location=device)\n","net.load_state_dict(checkpoint)\n","\n","\n","criterion = nn.CrossEntropyLoss()\n","feature_result = torch.tensor(0.)\n","total = torch.tensor(0.)"]},{"cell_type":"code","execution_count":11,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":33,"status":"ok","timestamp":1759254928007,"user":{"displayName":"Cccttt C","userId":"09161705153548643246"},"user_tz":-540},"id":"vE9V6s5werA2","outputId":"aceb31de-079c-49c6-ea7f-ab1371904ef5"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["Sequential(\n","  (conv0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","  (norm0): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","  (relu0): ReLU(inplace=True)\n","  (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","  (norm1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","  (relu1): ReLU(inplace=True)\n","  (pool2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n","  (conv3): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","  (norm3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","  (relu3): ReLU(inplace=True)\n","  (conv4): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","  (norm4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","  (relu4): ReLU(inplace=True)\n","  (pool5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n","  (conv6): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","  (norm6): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","  (relu6): ReLU(inplace=True)\n","  (conv7): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","  (norm7): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","  (relu7): ReLU(inplace=True)\n","  (conv8): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","  (norm8): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","  (relu8): ReLU(inplace=True)\n","  (pool9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n","  (conv10): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","  (norm10): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","  (relu10): ReLU(inplace=True)\n","  (conv11): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","  (norm11): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","  (relu11): ReLU(inplace=True)\n","  (conv12): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","  (norm12): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","  (relu12): ReLU(inplace=True)\n","  (pool13): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n","  (conv14): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","  (norm14): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","  (relu14): ReLU(inplace=True)\n","  (conv15): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","  (norm15): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","  (relu15): ReLU(inplace=True)\n","  (conv16): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n","  (norm16): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","  (relu16): ReLU(inplace=True)\n",")"]},"metadata":{},"execution_count":11}],"source":["net.features"]},{"cell_type":"code","execution_count":12,"metadata":{"executionInfo":{"elapsed":3,"status":"ok","timestamp":1759254928015,"user":{"displayName":"Cccttt C","userId":"09161705153548643246"},"user_tz":-540},"id":"fglEMODJR_vm"},"outputs":[],"source":["def get_feature_hook(self, input, output):\n","    global feature_result\n","    global entropy\n","    global total\n","    a = output.shape[0]     #batch size\n","    b = output.shape[1]     #the number of channel\n","    c = torch.tensor([\n","        torch.linalg.matrix_rank(output[i,j,:,:]).item()  #행렬 rank 계산\n","        for i in range(a) for j in range(b)])             #batch siez * the number of channel\n","\n","    #각 (배치, 채널)마다 2D rank 계산\n","\n","    c = c.view(a, -1).float()\n","    c = c.sum(0)\n","    feature_result = feature_result * total + c #rank 누적\n","    total = total + a\n","    feature_result = feature_result / total     #평균 rank 계산"]},{"cell_type":"code","execution_count":13,"metadata":{"executionInfo":{"elapsed":23,"status":"ok","timestamp":1759254928041,"user":{"displayName":"Cccttt C","userId":"09161705153548643246"},"user_tz":-540},"id":"UDA3DUzmQ5Hg"},"outputs":[],"source":["def test():\n","    global best_acc\n","    net.eval()\n","    test_loss = 0\n","    correct = 0\n","    total = 0\n","    limit = 20        #20번 실행\n","\n","    with torch.no_grad():\n","        for batch_idx, (inputs, targets) in enumerate(trainloader):\n","            if batch_idx >= limit:  # use the first 6 batches to estimate the rank.\n","               break\n","            inputs, targets = inputs.to(device), targets.to(device)\n","            outputs = net(inputs)\n","            loss = criterion(outputs, targets)\n","\n","            test_loss += loss.item()\n","            _, predicted = outputs.max(1)\n","            total += targets.size(0)\n","            correct += predicted.eq(targets).sum().item()\n","\n","            print('batch_idx: ' , batch_idx, limit, '\\nLoss: %.3f | Acc: %.3f%% (%d/%d)'\n","                % (test_loss/(batch_idx+1), 100.*correct/total, correct, total))#'''\n"]},{"cell_type":"code","execution_count":14,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"j00rLqfHR8yI","executionInfo":{"status":"ok","timestamp":1759258252965,"user_tz":-540,"elapsed":3324922,"user":{"displayName":"Cccttt C","userId":"09161705153548643246"}},"outputId":"5d19830a-8407-4641-cd60-fafafc3c79c4"},"outputs":[{"output_type":"stream","name":"stdout","text":["batch_idx:  0 20 \n","Loss: 0.280 | Acc: 94.531% (121/128)\n","batch_idx:  1 20 \n","Loss: 0.377 | Acc: 92.188% (236/256)\n","batch_idx:  2 20 \n","Loss: 0.437 | Acc: 91.146% (350/384)\n","batch_idx:  3 20 \n","Loss: 0.446 | Acc: 91.211% (467/512)\n","batch_idx:  4 20 \n","Loss: 0.455 | Acc: 91.250% (584/640)\n","batch_idx:  5 20 \n","Loss: 0.439 | Acc: 91.927% (706/768)\n","batch_idx:  6 20 \n","Loss: 0.445 | Acc: 91.853% (823/896)\n","batch_idx:  7 20 \n","Loss: 0.430 | Acc: 91.895% (941/1024)\n","batch_idx:  8 20 \n","Loss: 0.451 | Acc: 91.319% (1052/1152)\n","batch_idx:  9 20 \n","Loss: 0.444 | Acc: 91.484% (1171/1280)\n","batch_idx:  10 20 \n","Loss: 0.434 | Acc: 91.761% (1292/1408)\n","batch_idx:  11 20 \n","Loss: 0.430 | Acc: 91.927% (1412/1536)\n","batch_idx:  12 20 \n","Loss: 0.424 | Acc: 92.127% (1533/1664)\n","batch_idx:  13 20 \n","Loss: 0.432 | Acc: 91.853% (1646/1792)\n","batch_idx:  14 20 \n","Loss: 0.429 | Acc: 91.875% (1764/1920)\n","batch_idx:  15 20 \n","Loss: 0.436 | Acc: 91.650% (1877/2048)\n","batch_idx:  16 20 \n","Loss: 0.451 | Acc: 91.131% (1983/2176)\n","batch_idx:  17 20 \n","Loss: 0.447 | Acc: 91.276% (2103/2304)\n","batch_idx:  18 20 \n","Loss: 0.443 | Acc: 91.406% (2223/2432)\n","batch_idx:  19 20 \n","Loss: 0.446 | Acc: 91.367% (2339/2560)\n","saved the 1 layer\n","batch_idx:  0 20 \n","Loss: 0.400 | Acc: 92.969% (119/128)\n","batch_idx:  1 20 \n","Loss: 0.383 | Acc: 92.578% (237/256)\n","batch_idx:  2 20 \n","Loss: 0.365 | Acc: 92.448% (355/384)\n","batch_idx:  3 20 \n","Loss: 0.373 | Acc: 92.383% (473/512)\n","batch_idx:  4 20 \n","Loss: 0.392 | Acc: 91.875% (588/640)\n","batch_idx:  5 20 \n","Loss: 0.411 | Acc: 91.016% (699/768)\n","batch_idx:  6 20 \n","Loss: 0.409 | Acc: 91.518% (820/896)\n","batch_idx:  7 20 \n","Loss: 0.402 | Acc: 91.797% (940/1024)\n","batch_idx:  8 20 \n","Loss: 0.415 | Acc: 91.493% (1054/1152)\n","batch_idx:  9 20 \n","Loss: 0.409 | Acc: 91.406% (1170/1280)\n","batch_idx:  10 20 \n","Loss: 0.404 | Acc: 91.690% (1291/1408)\n","batch_idx:  11 20 \n","Loss: 0.403 | Acc: 91.797% (1410/1536)\n","batch_idx:  12 20 \n","Loss: 0.404 | Acc: 91.767% (1527/1664)\n","batch_idx:  13 20 \n","Loss: 0.402 | Acc: 91.741% (1644/1792)\n","batch_idx:  14 20 \n","Loss: 0.410 | Acc: 91.615% (1759/1920)\n","batch_idx:  15 20 \n","Loss: 0.413 | Acc: 91.455% (1873/2048)\n","batch_idx:  16 20 \n","Loss: 0.410 | Acc: 91.498% (1991/2176)\n","batch_idx:  17 20 \n","Loss: 0.411 | Acc: 91.493% (2108/2304)\n","batch_idx:  18 20 \n","Loss: 0.412 | Acc: 91.365% (2222/2432)\n","batch_idx:  19 20 \n","Loss: 0.409 | Acc: 91.562% (2344/2560)\n","saved the 2 layer\n","batch_idx:  0 20 \n","Loss: 0.481 | Acc: 89.844% (115/128)\n","batch_idx:  1 20 \n","Loss: 0.474 | Acc: 90.625% (232/256)\n","batch_idx:  2 20 \n","Loss: 0.427 | Acc: 91.667% (352/384)\n","batch_idx:  3 20 \n","Loss: 0.417 | Acc: 91.602% (469/512)\n","batch_idx:  4 20 \n","Loss: 0.424 | Acc: 91.250% (584/640)\n","batch_idx:  5 20 \n","Loss: 0.416 | Acc: 91.406% (702/768)\n","batch_idx:  6 20 \n","Loss: 0.448 | Acc: 90.737% (813/896)\n","batch_idx:  7 20 \n","Loss: 0.443 | Acc: 91.016% (932/1024)\n","batch_idx:  8 20 \n","Loss: 0.429 | Acc: 91.580% (1055/1152)\n","batch_idx:  9 20 \n","Loss: 0.430 | Acc: 91.406% (1170/1280)\n","batch_idx:  10 20 \n","Loss: 0.430 | Acc: 91.619% (1290/1408)\n","batch_idx:  11 20 \n","Loss: 0.431 | Acc: 91.602% (1407/1536)\n","batch_idx:  12 20 \n","Loss: 0.435 | Acc: 91.346% (1520/1664)\n","batch_idx:  13 20 \n","Loss: 0.431 | Acc: 91.518% (1640/1792)\n","batch_idx:  14 20 \n","Loss: 0.433 | Acc: 91.510% (1757/1920)\n","batch_idx:  15 20 \n","Loss: 0.433 | Acc: 91.455% (1873/2048)\n","batch_idx:  16 20 \n","Loss: 0.435 | Acc: 91.544% (1992/2176)\n","batch_idx:  17 20 \n","Loss: 0.428 | Acc: 91.753% (2114/2304)\n","batch_idx:  18 20 \n","Loss: 0.433 | Acc: 91.735% (2231/2432)\n","batch_idx:  19 20 \n","Loss: 0.439 | Acc: 91.680% (2347/2560)\n","saved the 3 layer\n","batch_idx:  0 20 \n","Loss: 0.403 | Acc: 92.188% (118/128)\n","batch_idx:  1 20 \n","Loss: 0.423 | Acc: 91.797% (235/256)\n","batch_idx:  2 20 \n","Loss: 0.447 | Acc: 90.625% (348/384)\n","batch_idx:  3 20 \n","Loss: 0.421 | Acc: 91.797% (470/512)\n","batch_idx:  4 20 \n","Loss: 0.449 | Acc: 90.938% (582/640)\n","batch_idx:  5 20 \n","Loss: 0.441 | Acc: 91.276% (701/768)\n","batch_idx:  6 20 \n","Loss: 0.454 | Acc: 91.071% (816/896)\n","batch_idx:  7 20 \n","Loss: 0.441 | Acc: 91.406% (936/1024)\n","batch_idx:  8 20 \n","Loss: 0.442 | Acc: 91.319% (1052/1152)\n","batch_idx:  9 20 \n","Loss: 0.460 | Acc: 90.625% (1160/1280)\n","batch_idx:  10 20 \n","Loss: 0.458 | Acc: 90.767% (1278/1408)\n","batch_idx:  11 20 \n","Loss: 0.455 | Acc: 90.755% (1394/1536)\n","batch_idx:  12 20 \n","Loss: 0.448 | Acc: 91.046% (1515/1664)\n","batch_idx:  13 20 \n","Loss: 0.446 | Acc: 91.127% (1633/1792)\n","batch_idx:  14 20 \n","Loss: 0.443 | Acc: 90.990% (1747/1920)\n","batch_idx:  15 20 \n","Loss: 0.435 | Acc: 91.113% (1866/2048)\n","batch_idx:  16 20 \n","Loss: 0.439 | Acc: 90.993% (1980/2176)\n","batch_idx:  17 20 \n","Loss: 0.438 | Acc: 91.102% (2099/2304)\n","batch_idx:  18 20 \n","Loss: 0.433 | Acc: 91.283% (2220/2432)\n","batch_idx:  19 20 \n","Loss: 0.429 | Acc: 91.406% (2340/2560)\n","saved the 4 layer\n","batch_idx:  0 20 \n","Loss: 0.497 | Acc: 90.625% (116/128)\n","batch_idx:  1 20 \n","Loss: 0.401 | Acc: 92.578% (237/256)\n","batch_idx:  2 20 \n","Loss: 0.436 | Acc: 91.406% (351/384)\n","batch_idx:  3 20 \n","Loss: 0.403 | Acc: 91.992% (471/512)\n","batch_idx:  4 20 \n","Loss: 0.404 | Acc: 92.188% (590/640)\n","batch_idx:  5 20 \n","Loss: 0.390 | Acc: 92.839% (713/768)\n","batch_idx:  6 20 \n","Loss: 0.390 | Acc: 92.522% (829/896)\n","batch_idx:  7 20 \n","Loss: 0.389 | Acc: 92.578% (948/1024)\n","batch_idx:  8 20 \n","Loss: 0.397 | Acc: 92.361% (1064/1152)\n","batch_idx:  9 20 \n","Loss: 0.396 | Acc: 92.266% (1181/1280)\n","batch_idx:  10 20 \n","Loss: 0.403 | Acc: 91.832% (1293/1408)\n","batch_idx:  11 20 \n","Loss: 0.398 | Acc: 92.057% (1414/1536)\n","batch_idx:  12 20 \n","Loss: 0.400 | Acc: 91.887% (1529/1664)\n","batch_idx:  13 20 \n","Loss: 0.406 | Acc: 91.629% (1642/1792)\n","batch_idx:  14 20 \n","Loss: 0.419 | Acc: 91.302% (1753/1920)\n","batch_idx:  15 20 \n","Loss: 0.419 | Acc: 91.260% (1869/2048)\n","batch_idx:  16 20 \n","Loss: 0.411 | Acc: 91.544% (1992/2176)\n","batch_idx:  17 20 \n","Loss: 0.410 | Acc: 91.667% (2112/2304)\n","batch_idx:  18 20 \n","Loss: 0.419 | Acc: 91.447% (2224/2432)\n","batch_idx:  19 20 \n","Loss: 0.418 | Acc: 91.445% (2341/2560)\n","saved the 5 layer\n","batch_idx:  0 20 \n","Loss: 0.553 | Acc: 88.281% (113/128)\n","batch_idx:  1 20 \n","Loss: 0.488 | Acc: 90.234% (231/256)\n","batch_idx:  2 20 \n","Loss: 0.511 | Acc: 89.323% (343/384)\n","batch_idx:  3 20 \n","Loss: 0.475 | Acc: 90.234% (462/512)\n","batch_idx:  4 20 \n","Loss: 0.467 | Acc: 90.312% (578/640)\n","batch_idx:  5 20 \n","Loss: 0.467 | Acc: 89.974% (691/768)\n","batch_idx:  6 20 \n","Loss: 0.464 | Acc: 89.844% (805/896)\n","batch_idx:  7 20 \n","Loss: 0.468 | Acc: 90.039% (922/1024)\n","batch_idx:  8 20 \n","Loss: 0.451 | Acc: 90.538% (1043/1152)\n","batch_idx:  9 20 \n","Loss: 0.442 | Acc: 90.859% (1163/1280)\n","batch_idx:  10 20 \n","Loss: 0.445 | Acc: 90.838% (1279/1408)\n","batch_idx:  11 20 \n","Loss: 0.441 | Acc: 90.951% (1397/1536)\n","batch_idx:  12 20 \n","Loss: 0.441 | Acc: 90.925% (1513/1664)\n","batch_idx:  13 20 \n","Loss: 0.440 | Acc: 90.792% (1627/1792)\n","batch_idx:  14 20 \n","Loss: 0.425 | Acc: 91.198% (1751/1920)\n","batch_idx:  15 20 \n","Loss: 0.423 | Acc: 91.211% (1868/2048)\n","batch_idx:  16 20 \n","Loss: 0.417 | Acc: 91.360% (1988/2176)\n","batch_idx:  17 20 \n","Loss: 0.413 | Acc: 91.493% (2108/2304)\n","batch_idx:  18 20 \n","Loss: 0.410 | Acc: 91.612% (2228/2432)\n","batch_idx:  19 20 \n","Loss: 0.408 | Acc: 91.719% (2348/2560)\n","saved the 6 layer\n","batch_idx:  0 20 \n","Loss: 0.367 | Acc: 92.188% (118/128)\n","batch_idx:  1 20 \n","Loss: 0.397 | Acc: 92.188% (236/256)\n","batch_idx:  2 20 \n","Loss: 0.414 | Acc: 92.188% (354/384)\n","batch_idx:  3 20 \n","Loss: 0.455 | Acc: 91.211% (467/512)\n","batch_idx:  4 20 \n","Loss: 0.458 | Acc: 91.250% (584/640)\n","batch_idx:  5 20 \n","Loss: 0.433 | Acc: 91.667% (704/768)\n","batch_idx:  6 20 \n","Loss: 0.444 | Acc: 91.071% (816/896)\n","batch_idx:  7 20 \n","Loss: 0.437 | Acc: 91.309% (935/1024)\n","batch_idx:  8 20 \n","Loss: 0.440 | Acc: 91.233% (1051/1152)\n","batch_idx:  9 20 \n","Loss: 0.432 | Acc: 91.406% (1170/1280)\n","batch_idx:  10 20 \n","Loss: 0.426 | Acc: 91.335% (1286/1408)\n","batch_idx:  11 20 \n","Loss: 0.434 | Acc: 91.211% (1401/1536)\n","batch_idx:  12 20 \n","Loss: 0.443 | Acc: 91.046% (1515/1664)\n","batch_idx:  13 20 \n","Loss: 0.444 | Acc: 91.016% (1631/1792)\n","batch_idx:  14 20 \n","Loss: 0.444 | Acc: 91.042% (1748/1920)\n","batch_idx:  15 20 \n","Loss: 0.445 | Acc: 91.016% (1864/2048)\n","batch_idx:  16 20 \n","Loss: 0.449 | Acc: 90.855% (1977/2176)\n","batch_idx:  17 20 \n","Loss: 0.446 | Acc: 90.842% (2093/2304)\n","batch_idx:  18 20 \n","Loss: 0.439 | Acc: 90.954% (2212/2432)\n","batch_idx:  19 20 \n","Loss: 0.444 | Acc: 90.781% (2324/2560)\n","saved the 7 layer\n","batch_idx:  0 20 \n","Loss: 0.521 | Acc: 86.719% (111/128)\n","batch_idx:  1 20 \n","Loss: 0.436 | Acc: 89.844% (230/256)\n","batch_idx:  2 20 \n","Loss: 0.454 | Acc: 89.583% (344/384)\n","batch_idx:  3 20 \n","Loss: 0.457 | Acc: 89.844% (460/512)\n","batch_idx:  4 20 \n","Loss: 0.453 | Acc: 90.156% (577/640)\n","batch_idx:  5 20 \n","Loss: 0.459 | Acc: 89.974% (691/768)\n","batch_idx:  6 20 \n","Loss: 0.461 | Acc: 89.955% (806/896)\n","batch_idx:  7 20 \n","Loss: 0.461 | Acc: 90.137% (923/1024)\n","batch_idx:  8 20 \n","Loss: 0.449 | Acc: 90.278% (1040/1152)\n","batch_idx:  9 20 \n","Loss: 0.436 | Acc: 90.859% (1163/1280)\n","batch_idx:  10 20 \n","Loss: 0.430 | Acc: 90.980% (1281/1408)\n","batch_idx:  11 20 \n","Loss: 0.431 | Acc: 90.951% (1397/1536)\n","batch_idx:  12 20 \n","Loss: 0.427 | Acc: 91.106% (1516/1664)\n","batch_idx:  13 20 \n","Loss: 0.424 | Acc: 91.127% (1633/1792)\n","batch_idx:  14 20 \n","Loss: 0.429 | Acc: 90.938% (1746/1920)\n","batch_idx:  15 20 \n","Loss: 0.426 | Acc: 91.016% (1864/2048)\n","batch_idx:  16 20 \n","Loss: 0.439 | Acc: 90.579% (1971/2176)\n","batch_idx:  17 20 \n","Loss: 0.438 | Acc: 90.625% (2088/2304)\n","batch_idx:  18 20 \n","Loss: 0.445 | Acc: 90.543% (2202/2432)\n","batch_idx:  19 20 \n","Loss: 0.443 | Acc: 90.547% (2318/2560)\n","saved the 8 layer\n","batch_idx:  0 20 \n","Loss: 0.395 | Acc: 92.188% (118/128)\n","batch_idx:  1 20 \n","Loss: 0.412 | Acc: 92.188% (236/256)\n","batch_idx:  2 20 \n","Loss: 0.442 | Acc: 92.188% (354/384)\n","batch_idx:  3 20 \n","Loss: 0.430 | Acc: 91.992% (471/512)\n","batch_idx:  4 20 \n","Loss: 0.419 | Acc: 91.719% (587/640)\n","batch_idx:  5 20 \n","Loss: 0.455 | Acc: 90.495% (695/768)\n","batch_idx:  6 20 \n","Loss: 0.430 | Acc: 91.183% (817/896)\n","batch_idx:  7 20 \n","Loss: 0.430 | Acc: 91.309% (935/1024)\n","batch_idx:  8 20 \n","Loss: 0.438 | Acc: 91.146% (1050/1152)\n","batch_idx:  9 20 \n","Loss: 0.438 | Acc: 91.094% (1166/1280)\n","batch_idx:  10 20 \n","Loss: 0.440 | Acc: 90.980% (1281/1408)\n","batch_idx:  11 20 \n","Loss: 0.457 | Acc: 90.755% (1394/1536)\n","batch_idx:  12 20 \n","Loss: 0.449 | Acc: 90.986% (1514/1664)\n","batch_idx:  13 20 \n","Loss: 0.442 | Acc: 91.071% (1632/1792)\n","batch_idx:  14 20 \n","Loss: 0.439 | Acc: 91.146% (1750/1920)\n","batch_idx:  15 20 \n","Loss: 0.435 | Acc: 91.260% (1869/2048)\n","batch_idx:  16 20 \n","Loss: 0.436 | Acc: 91.131% (1983/2176)\n","batch_idx:  17 20 \n","Loss: 0.435 | Acc: 91.233% (2102/2304)\n","batch_idx:  18 20 \n","Loss: 0.434 | Acc: 91.160% (2217/2432)\n","batch_idx:  19 20 \n","Loss: 0.438 | Acc: 91.055% (2331/2560)\n","saved the 9 layer\n","batch_idx:  0 20 \n","Loss: 0.315 | Acc: 93.750% (120/128)\n","batch_idx:  1 20 \n","Loss: 0.361 | Acc: 92.188% (236/256)\n","batch_idx:  2 20 \n","Loss: 0.351 | Acc: 92.448% (355/384)\n","batch_idx:  3 20 \n","Loss: 0.388 | Acc: 91.797% (470/512)\n","batch_idx:  4 20 \n","Loss: 0.397 | Acc: 91.719% (587/640)\n","batch_idx:  5 20 \n","Loss: 0.409 | Acc: 91.146% (700/768)\n","batch_idx:  6 20 \n","Loss: 0.396 | Acc: 91.629% (821/896)\n","batch_idx:  7 20 \n","Loss: 0.403 | Acc: 91.504% (937/1024)\n","batch_idx:  8 20 \n","Loss: 0.407 | Acc: 91.406% (1053/1152)\n","batch_idx:  9 20 \n","Loss: 0.406 | Acc: 91.484% (1171/1280)\n","batch_idx:  10 20 \n","Loss: 0.395 | Acc: 91.903% (1294/1408)\n","batch_idx:  11 20 \n","Loss: 0.399 | Acc: 91.797% (1410/1536)\n","batch_idx:  12 20 \n","Loss: 0.405 | Acc: 91.827% (1528/1664)\n","batch_idx:  13 20 \n","Loss: 0.401 | Acc: 92.132% (1651/1792)\n","batch_idx:  14 20 \n","Loss: 0.412 | Acc: 91.771% (1762/1920)\n","batch_idx:  15 20 \n","Loss: 0.410 | Acc: 91.797% (1880/2048)\n","batch_idx:  16 20 \n","Loss: 0.418 | Acc: 91.590% (1993/2176)\n","batch_idx:  17 20 \n","Loss: 0.412 | Acc: 91.667% (2112/2304)\n","batch_idx:  18 20 \n","Loss: 0.413 | Acc: 91.571% (2227/2432)\n","batch_idx:  19 20 \n","Loss: 0.419 | Acc: 91.367% (2339/2560)\n","saved the 10 layer\n","batch_idx:  0 20 \n","Loss: 0.560 | Acc: 86.719% (111/128)\n","batch_idx:  1 20 \n","Loss: 0.481 | Acc: 89.062% (228/256)\n","batch_idx:  2 20 \n","Loss: 0.397 | Acc: 91.927% (353/384)\n","batch_idx:  3 20 \n","Loss: 0.420 | Acc: 91.602% (469/512)\n","batch_idx:  4 20 \n","Loss: 0.428 | Acc: 91.250% (584/640)\n","batch_idx:  5 20 \n","Loss: 0.423 | Acc: 91.536% (703/768)\n","batch_idx:  6 20 \n","Loss: 0.424 | Acc: 91.741% (822/896)\n","batch_idx:  7 20 \n","Loss: 0.411 | Acc: 91.992% (942/1024)\n","batch_idx:  8 20 \n","Loss: 0.399 | Acc: 92.274% (1063/1152)\n","batch_idx:  9 20 \n","Loss: 0.390 | Acc: 92.500% (1184/1280)\n","batch_idx:  10 20 \n","Loss: 0.400 | Acc: 92.330% (1300/1408)\n","batch_idx:  11 20 \n","Loss: 0.405 | Acc: 92.122% (1415/1536)\n","batch_idx:  12 20 \n","Loss: 0.412 | Acc: 92.007% (1531/1664)\n","batch_idx:  13 20 \n","Loss: 0.409 | Acc: 92.020% (1649/1792)\n","batch_idx:  14 20 \n","Loss: 0.425 | Acc: 91.562% (1758/1920)\n","batch_idx:  15 20 \n","Loss: 0.418 | Acc: 91.797% (1880/2048)\n","batch_idx:  16 20 \n","Loss: 0.413 | Acc: 91.958% (2001/2176)\n","batch_idx:  17 20 \n","Loss: 0.411 | Acc: 91.970% (2119/2304)\n","batch_idx:  18 20 \n","Loss: 0.413 | Acc: 91.941% (2236/2432)\n","batch_idx:  19 20 \n","Loss: 0.414 | Acc: 91.992% (2355/2560)\n","saved the 11 layer\n","batch_idx:  0 20 \n","Loss: 0.593 | Acc: 85.938% (110/128)\n","batch_idx:  1 20 \n","Loss: 0.471 | Acc: 89.844% (230/256)\n","batch_idx:  2 20 \n","Loss: 0.488 | Acc: 89.844% (345/384)\n","batch_idx:  3 20 \n","Loss: 0.461 | Acc: 90.820% (465/512)\n","batch_idx:  4 20 \n","Loss: 0.478 | Acc: 90.000% (576/640)\n","batch_idx:  5 20 \n","Loss: 0.461 | Acc: 90.625% (696/768)\n","batch_idx:  6 20 \n","Loss: 0.450 | Acc: 91.071% (816/896)\n","batch_idx:  7 20 \n","Loss: 0.445 | Acc: 91.113% (933/1024)\n","batch_idx:  8 20 \n","Loss: 0.441 | Acc: 91.059% (1049/1152)\n","batch_idx:  9 20 \n","Loss: 0.448 | Acc: 90.703% (1161/1280)\n","batch_idx:  10 20 \n","Loss: 0.450 | Acc: 90.554% (1275/1408)\n","batch_idx:  11 20 \n","Loss: 0.448 | Acc: 90.690% (1393/1536)\n","batch_idx:  12 20 \n","Loss: 0.437 | Acc: 90.925% (1513/1664)\n","batch_idx:  13 20 \n","Loss: 0.447 | Acc: 90.681% (1625/1792)\n","batch_idx:  14 20 \n","Loss: 0.440 | Acc: 90.885% (1745/1920)\n","batch_idx:  15 20 \n","Loss: 0.440 | Acc: 90.918% (1862/2048)\n","batch_idx:  16 20 \n","Loss: 0.455 | Acc: 90.579% (1971/2176)\n","batch_idx:  17 20 \n","Loss: 0.460 | Acc: 90.495% (2085/2304)\n","batch_idx:  18 20 \n","Loss: 0.463 | Acc: 90.296% (2196/2432)\n","batch_idx:  19 20 \n","Loss: 0.459 | Acc: 90.391% (2314/2560)\n","saved the 12 layer\n","batch_idx:  0 20 \n","Loss: 0.534 | Acc: 86.719% (111/128)\n","batch_idx:  1 20 \n","Loss: 0.452 | Acc: 89.062% (228/256)\n","batch_idx:  2 20 \n","Loss: 0.425 | Acc: 90.104% (346/384)\n","batch_idx:  3 20 \n","Loss: 0.398 | Acc: 90.625% (464/512)\n","batch_idx:  4 20 \n","Loss: 0.418 | Acc: 90.625% (580/640)\n","batch_idx:  5 20 \n","Loss: 0.428 | Acc: 89.974% (691/768)\n","batch_idx:  6 20 \n","Loss: 0.405 | Acc: 90.848% (814/896)\n","batch_idx:  7 20 \n","Loss: 0.406 | Acc: 90.723% (929/1024)\n","batch_idx:  8 20 \n","Loss: 0.397 | Acc: 90.972% (1048/1152)\n","batch_idx:  9 20 \n","Loss: 0.400 | Acc: 91.172% (1167/1280)\n","batch_idx:  10 20 \n","Loss: 0.397 | Acc: 91.335% (1286/1408)\n","batch_idx:  11 20 \n","Loss: 0.404 | Acc: 91.211% (1401/1536)\n","batch_idx:  12 20 \n","Loss: 0.410 | Acc: 91.226% (1518/1664)\n","batch_idx:  13 20 \n","Loss: 0.401 | Acc: 91.350% (1637/1792)\n","batch_idx:  14 20 \n","Loss: 0.410 | Acc: 91.198% (1751/1920)\n","batch_idx:  15 20 \n","Loss: 0.408 | Acc: 91.406% (1872/2048)\n","batch_idx:  16 20 \n","Loss: 0.415 | Acc: 91.268% (1986/2176)\n","batch_idx:  17 20 \n","Loss: 0.419 | Acc: 91.102% (2099/2304)\n","batch_idx:  18 20 \n","Loss: 0.416 | Acc: 91.077% (2215/2432)\n","batch_idx:  19 20 \n","Loss: 0.418 | Acc: 91.172% (2334/2560)\n","saved the 13 layer\n"]}],"source":["from timeit import default_timer as time\n","\n","save_dir = origin_dir + '/rank_generation/'\n","\n","for i, cov_id in enumerate(relucfg):\n","    cov_layer = net.features[cov_id]\n","    handler = cov_layer.register_forward_hook(get_feature_hook) #forward 호출 시 get_feature_hook 호출\n","    test()              #loss 계산 / rank계산 확인용\n","    handler.remove()\n","\n","    np.save(save_dir+'/rank_conv' + str(i + 1) + '.npy', feature_result.numpy())\n","    print('saved the',str(i + 1),'layer')\n","    feature_result = torch.tensor(0.)\n","    total = torch.tensor(0.)"]},{"cell_type":"markdown","metadata":{"id":"6EWGQ1cRjVNr"},"source":["각 relu 포인트에 register_forward_hook(get_feature_hook) 검\n","test에서 forward 실행될때마다 get_feature_hook 실행\n","limit횟수만큼 forward\n","rank 계산 누적 및 learning 평균 업데이트"]},{"cell_type":"code","source":[],"metadata":{"id":"jbGljH5lwFkD","executionInfo":{"status":"ok","timestamp":1759258252970,"user_tz":-540,"elapsed":39,"user":{"displayName":"Cccttt C","userId":"09161705153548643246"}}},"execution_count":14,"outputs":[]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[],"authorship_tag":"ABX9TyP8N6uc6ZM74Ritq2mBCIAk"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}